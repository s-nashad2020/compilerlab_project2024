{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0PwYGm_pR4X",
        "outputId": "740fb1c3-713c-49a9-8094-3032e2de0591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your code:\n",
            "def add(a, b):     return a + b  def subtract(a, b):     return a - b  def multiply(a, b):     return a * b  def divide(a, b):     return a / b  x = 10 y = 5 if x > y:     print(\"x is greater than y\") else:     print(\"x is not greater than y\")  while x > 0:     print(x)     x = x - 1  print(\"End of program\")\n",
            "Tokens: [('def', 'DEF'), ('add', 'IDENTIFIER'), ('(', 'LPAREN'), ('a', 'IDENTIFIER'), (',', 'COMMA'), ('b', 'IDENTIFIER'), (')', 'RPAREN'), (':', 'COLON'), ('return', 'RETURN'), ('a', 'IDENTIFIER'), ('+', 'PLUS'), ('b', 'IDENTIFIER'), ('def', 'DEF'), ('subtract', 'IDENTIFIER'), ('(', 'LPAREN'), ('a', 'IDENTIFIER'), (',', 'COMMA'), ('b', 'IDENTIFIER'), (')', 'RPAREN'), (':', 'COLON'), ('return', 'RETURN'), ('a', 'IDENTIFIER'), ('-', 'MINUS'), ('b', 'IDENTIFIER'), ('def', 'DEF'), ('multiply', 'IDENTIFIER'), ('(', 'LPAREN'), ('a', 'IDENTIFIER'), (',', 'COMMA'), ('b', 'IDENTIFIER'), (')', 'RPAREN'), (':', 'COLON'), ('return', 'RETURN'), ('a', 'IDENTIFIER'), ('*', 'MULTIPLY'), ('b', 'IDENTIFIER'), ('def', 'DEF'), ('divide', 'IDENTIFIER'), ('(', 'LPAREN'), ('a', 'IDENTIFIER'), (',', 'COMMA'), ('b', 'IDENTIFIER'), (')', 'RPAREN'), (':', 'COLON'), ('return', 'RETURN'), ('a', 'IDENTIFIER'), ('/', 'DIVIDE'), ('b', 'IDENTIFIER'), ('x', 'IDENTIFIER'), ('=', 'ASSIGN'), ('10', 'NUMBER'), ('y', 'IDENTIFIER'), ('=', 'ASSIGN'), ('5', 'NUMBER'), ('if', 'IF'), ('x', 'IDENTIFIER'), ('>', 'GREATER_THAN'), ('y', 'IDENTIFIER'), (':', 'COLON'), ('print', 'PRINT'), ('(', 'LPAREN'), ('\"', 'INVALID'), ('x', 'IDENTIFIER'), ('is', 'IDENTIFIER'), ('greater', 'IDENTIFIER'), ('than', 'IDENTIFIER'), ('y', 'IDENTIFIER'), ('\"', 'INVALID'), (')', 'RPAREN'), ('else', 'ELSE'), (':', 'COLON'), ('print', 'PRINT'), ('(', 'LPAREN'), ('\"', 'INVALID'), ('x', 'IDENTIFIER'), ('is', 'IDENTIFIER'), ('not', 'IDENTIFIER'), ('greater', 'IDENTIFIER'), ('than', 'IDENTIFIER'), ('y', 'IDENTIFIER'), ('\"', 'INVALID'), (')', 'RPAREN'), ('while', 'WHILE'), ('x', 'IDENTIFIER'), ('>', 'GREATER_THAN'), ('0', 'NUMBER'), (':', 'COLON'), ('print', 'PRINT'), ('(', 'LPAREN'), ('x', 'IDENTIFIER'), (')', 'RPAREN'), ('x', 'IDENTIFIER'), ('=', 'ASSIGN'), ('x', 'IDENTIFIER'), ('-', 'MINUS'), ('1', 'NUMBER'), ('print', 'PRINT'), ('(', 'LPAREN'), ('\"', 'INVALID'), ('End', 'IDENTIFIER'), ('of', 'IDENTIFIER'), ('program', 'IDENTIFIER'), ('\"', 'INVALID'), (')', 'RPAREN')]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        self.tokens = []\n",
        "\n",
        "    def tokenize_input(self, input_string):\n",
        "        # Define regex patterns for tokens\n",
        "        patterns = [\n",
        "            (r'def', 'DEF'),                # Match '='\n",
        "            (r'if', 'IF'),                   # Match 'if'\n",
        "            (r'else', 'ELSE'),               # Match 'else'\n",
        "            (r'while', 'WHILE'),             # Match 'while'\n",
        "            (r'return', 'RETURN'),           # Match 'return'\n",
        "            (r'print', 'PRINT'),                   # Match 'def'\n",
        "            (r'[a-zA-Z_][a-zA-Z0-9_]*', 'IDENTIFIER'),  # Match identifiers\n",
        "            (r'\\d+', 'NUMBER'),              # Match numbers\n",
        "            (r'\\(', 'LPAREN'),               # Match '('\n",
        "            (r'\\)', 'RPAREN'),               # Match ')'\n",
        "            (r':', 'COLON'),                 # Match ':'\n",
        "            (r',', 'COMMA'),                 # Match ','\n",
        "            (r'\\+', 'PLUS'),                 # Match '+'\n",
        "            (r'-', 'MINUS'),                 # Match '-'\n",
        "            (r'\\*', 'MULTIPLY'),             # Match '*'\n",
        "            (r'/', 'DIVIDE'),                # Match '/'\n",
        "            (r'=', 'ASSIGN'),           # Match 'print'\n",
        "            (r'>', 'GREATER_THAN'),          # Match '>'\n",
        "            (r'<', 'LESS_THAN'),             # Match '<'\n",
        "            (r'\\n', 'NEWLINE'),              # Match newline\n",
        "            (r'\\s+', None),                  # Match whitespace (ignore)\n",
        "            (r'.', 'INVALID')                # Match any other character (invalid)\n",
        "        ]\n",
        "\n",
        "        # Tokenize input string\n",
        "        while input_string:\n",
        "            found_match = False\n",
        "            for pattern, token_type in patterns:\n",
        "                match = re.match(pattern, input_string)\n",
        "                if match:\n",
        "                    found_match = True\n",
        "                    if token_type:\n",
        "                        self.tokens.append((match.group(), token_type))\n",
        "                    input_string = input_string[match.end():]\n",
        "                    break\n",
        "            if not found_match:\n",
        "                raise SyntaxError(\"Invalid input\")\n",
        "\n",
        "    def accept_input(self):\n",
        "        try:\n",
        "            input_string = input(\"Enter your code:\\n\")\n",
        "            self.tokenize_input(input_string)\n",
        "            print(\"Tokens:\", self.tokens)\n",
        "        except SyntaxError as e:\n",
        "            print(\"Error:\", e)\n",
        "\n",
        "# Example usage\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.accept_input()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lh-ysS50kRXw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}